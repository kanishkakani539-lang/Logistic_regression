# -*- coding: utf-8 -*-
"""Untitled17.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PjxVrDZUVyARYjxZARthkHrPtQI7hmQv
"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_blobs

# 1. Dataset Generation
X, y = make_blobs(
    n_samples=100,
    n_features=2,
    centers=2,
    cluster_std=1.0,
    random_state=42
)
y = y.reshape(-1, 1)

# Sigmoid function
def sigmoid(z):
    return 1 / (1 + np.exp(-z))

# Binary cross entropy loss
def compute_cost(X, y, W, b):
    m = len(y)
    z = X.dot(W) + b
    p = sigmoid(z)
    cost = - (1/m) * np.sum(
        y * np.log(p + 1e-10) +
        (1 - y) * np.log(1 - p + 1e-10)
    )
    return cost

# Gradient descent step
def gradient_step(X, y, W, b, lr):
    m = len(y)
    z = X.dot(W) + b
    p = sigmoid(z)
    dW = (1/m) * X.T.dot(p - y)
    db = (1/m) * np.sum(p - y)
    W = W - lr * dW
    b = b - lr * db
    return W, b

# 2. Training Loop
W = np.zeros((2, 1))
b = 0
learning_rate = 0.05
iterations = 1000
cost_history = []

for i in range(iterations):
    W, b = gradient_step(X, y, W, b, learning_rate)
    if i % 100 == 0:
        cost_history.append(compute_cost(X, y, W, b))

final_cost = compute_cost(X, y, W, b)
W1 = float(W[0])
W2 = float(W[1])
bias = float(b)

print("Final Cost:", final_cost)
print("W1:", W1)
print("W2:", W2)
print("Bias:", bias)

# 3. Cost Plot
plt.plot(cost_history)
plt.title("Cost Convergence")
plt.xlabel("Checkpoints")
plt.ylabel("Cost")
plt.grid(True)
plt.savefig("cost_plot.png")
plt.close()

# 4. Decision Boundary Plot
plt.scatter(X[:,0], X[:,1], c=y.reshape(-1))
x1 = np.linspace(X[:,0].min()-1, X[:,0].max()+1, 200)
x2 = -(W[0]*x1 + b)/W[1]
plt.plot(x1, x2)
plt.title("Decision Boundary")
plt.xlabel("Feature 1")
plt.ylabel("Feature 2")
plt.grid(True)
plt.savefig("decision_boundary.png")
plt.close()